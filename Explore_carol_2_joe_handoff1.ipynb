{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import RFE, SelectKBest, f_regression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, auc, roc_curve, recall_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from imblearn.ensemble import BalancedBaggingClassifier \n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "weather = pd.read_csv('weather.csv')\n",
    "spray = pd.read_csv('spray.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10506 entries, 0 to 10505\n",
      "Data columns (total 12 columns):\n",
      "Date                      10506 non-null object\n",
      "Address                   10506 non-null object\n",
      "Species                   10506 non-null object\n",
      "Block                     10506 non-null int64\n",
      "Street                    10506 non-null object\n",
      "Trap                      10506 non-null object\n",
      "AddressNumberAndStreet    10506 non-null object\n",
      "Latitude                  10506 non-null float64\n",
      "Longitude                 10506 non-null float64\n",
      "AddressAccuracy           10506 non-null int64\n",
      "NumMosquitos              10506 non-null int64\n",
      "WnvPresent                10506 non-null int64\n",
      "dtypes: float64(2), int64(4), object(6)\n",
      "memory usage: 985.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 116293 entries, 0 to 116292\n",
      "Data columns (total 11 columns):\n",
      "Id                        116293 non-null int64\n",
      "Date                      116293 non-null object\n",
      "Address                   116293 non-null object\n",
      "Species                   116293 non-null object\n",
      "Block                     116293 non-null int64\n",
      "Street                    116293 non-null object\n",
      "Trap                      116293 non-null object\n",
      "AddressNumberAndStreet    116293 non-null object\n",
      "Latitude                  116293 non-null float64\n",
      "Longitude                 116293 non-null float64\n",
      "AddressAccuracy           116293 non-null int64\n",
      "dtypes: float64(2), int64(3), object(6)\n",
      "memory usage: 9.8+ MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_train_data(df):\n",
    "    \n",
    "    # map df dataset with weather dataset\n",
    "    mask3 = np.power((df['Latitude']-41.995),2)+ np.power((df['Longitude']-(-87.933)),2) <= \\\n",
    "            np.power((df['Latitude']-41.786),2)+ np.power((df['Longitude']-(-87.752)),2)\n",
    "    mask4 = np.power((df['Latitude']-41.995),2)+ np.power((df['Longitude']-(-87.933)),2) > \\\n",
    "            np.power((df['Latitude']-41.786),2)+ np.power((df['Longitude']-(-87.752)),2)\n",
    "    df.loc[mask3,'weather_station'] = 1\n",
    "    df.loc[mask4,'weather_station'] = 2\n",
    "    df_weather = df.join(weather.set_index(['Date','Station']), on=(['Date','weather_station']), how='left')\n",
    "    \n",
    "    # map df_weather dataset with spray dataset\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    date_formated_df = []\n",
    "    date_formated_spray = []\n",
    "    for i in df_weather['Date']:\n",
    "        date_formated_df.append(datetime.strptime(i, date_format))\n",
    "    for i in spray['Date']:\n",
    "        date_formated_spray.append(datetime.strptime(i, date_format))\n",
    "    date_formated_spray = set(date_formated_spray)\n",
    "    date_formated_spray = list(date_formated_spray)\n",
    "    days_after_spray = []\n",
    "    for i in date_formated_df:\n",
    "        delta = []\n",
    "        for n in date_formated_spray:\n",
    "            if (i - n).days > 0:\n",
    "                delta.append((i - n).days)\n",
    "        if len(delta) != 0:\n",
    "            days = np.min(delta)\n",
    "        else:\n",
    "            days = 0\n",
    "        days_after_spray.append(days)\n",
    "    df_weather['days_after_spray'] = days_after_spray\n",
    "\n",
    "    ## deal with the missing value of the useful columns\n",
    "    mask5 = (df_weather.PrecipTotal == '  T')| (df_weather.PrecipTotal =='M')\n",
    "    mask6 = df_weather.WetBulb == 'M'\n",
    "    mask7 = df_weather.StnPressure == 'M'\n",
    "    df_weather.loc[mask5,'PrecipTotal'] = 0.00\n",
    "    df_weather.loc[mask6,'WetBulb'] = round((df_weather[df_weather.WetBulb!='M']['WetBulb']).astype('int').mean())\n",
    "    df_weather.loc[mask7,'StnPressure'] = round((df_weather[df_weather.StnPressure!='M']['StnPressure']).astype('float').mean(),2)\n",
    "    \n",
    "    #create columns for light and heavy rain day off and within 14 days \n",
    "    df_weather['PrecipTotal'] = df_weather['PrecipTotal'].astype(float)\n",
    "    precip_75quantile = df_weather[df_weather['PrecipTotal'] > 0]['PrecipTotal'].quantile(.75)\n",
    "    precip_25quantile = df_weather[df_weather['PrecipTotal'] > 0]['PrecipTotal'].quantile(.25)\n",
    "\n",
    "    df_weather['heavy_rain'] = (df_weather['PrecipTotal']>precip_75quantile).astype(int)\n",
    "    df_weather['light_rain'] = ((df_weather['PrecipTotal']>0) & (df_weather['PrecipTotal']<precip_25quantile)).astype(int)\n",
    "\n",
    "    for i in range(1,15):\n",
    "        df_weather['light_rain_past' + str(i)] = df_weather['light_rain'].shift(periods=i)\n",
    "        df_weather['heavy_rain_past' + str(i)] = df_weather['heavy_rain'].shift(periods=i)\n",
    "    \n",
    "    #create column if it rained in the past 14 days \n",
    "    df_weather['heavy_rain_last_14_days'] = np.where(df_weather[['heavy_rain_past1', 'heavy_rain_past2',\n",
    "       'heavy_rain_past3', 'heavy_rain_past4', 'heavy_rain_past5',\n",
    "       'heavy_rain_past6', 'heavy_rain_past7', 'heavy_rain_past8',\n",
    "       'heavy_rain_past9', 'heavy_rain_past10', 'heavy_rain_past11',\n",
    "       'heavy_rain_past12', 'heavy_rain_past13', 'heavy_rain_past14']].any(axis=1), 1, 0)\n",
    "\n",
    "    df_weather['light_reain_last_14_days'] = np.where(df_weather[['light_rain_past1', 'light_rain_past2',\n",
    "       'light_rain_past3', 'light_rain_past4', 'light_rain_past5',\n",
    "       'light_rain_past6', 'light_rain_past7', 'light_rain_past8',\n",
    "       'light_rain_past9', 'light_rain_past10', 'light_rain_past11',\n",
    "       'light_rain_past12', 'light_rain_past13', 'light_rain_past14',]].any(axis=1), 1, 0)\n",
    "    \n",
    "    # clean up the total dataset \n",
    "    df_total = df_weather\n",
    "    \n",
    "    #fix NumMosquitos column\n",
    "    df_total['NumMosquitos_sum'] = np.nan #create sum column\n",
    "    #fill sum column with sum of matching rows\n",
    "    df_total['NumMosquitos_sum'].fillna(df_total.groupby(['Date','Trap','Species'])['NumMosquitos'].transform('sum'), inplace=True)\n",
    "    df_total.drop(['NumMosquitos'], axis=1, inplace=True) #drop old column\n",
    "    df_total.drop_duplicates(inplace=True) #drop duplicated rows \n",
    "    df_total.reset_index(inplace=True) #reset index \n",
    "    \n",
    "    #create ordinal date column\n",
    "    df_total['Date'] = pd.to_datetime(df_total['Date'])\n",
    "    df_total['ordinal_date'] = df_total['Date'].map(lambda x: x.toordinal())\n",
    "    \n",
    "    #create month/day for time of year \n",
    "    df_total['month_day'] = df_total['Date'].dt.month + df_total['Date'].map(lambda x: .5 if x.day > 15 else 0)\n",
    "    \n",
    "    ## drop useless columns\n",
    "    ## different for train and test dataset because test data doesn't include 'NumMosquitos' column\n",
    "    ## and test data has \"Id\" column that need to be dropped\n",
    "    \n",
    "    df_total = df_total.drop(['Date','Address','Block','Street','Trap','AddressNumberAndStreet',\\\n",
    "              'AddressAccuracy','weather_station','Depart','CodeSum','Depth',\\\n",
    "              'Water1','SnowFall', 'Sunrise', 'Sunset', 'index'],axis=1)\n",
    "              # NumMosquitos and CodeSum may still helpful (test data doesn't include 'NumMosquitos' column)\n",
    "              #**changed this to keep NumMosquitos\n",
    "    \n",
    "    ## change the datatype to correct one\n",
    "    df_total[['Tavg','WetBulb','Heat','Cool']] = df_total[['Tavg','WetBulb','Heat','Cool']].astype(int)\n",
    "    df_total[['PrecipTotal','StnPressure','SeaLevel','AvgSpeed']] = \\\n",
    "    df_total[['PrecipTotal','StnPressure','SeaLevel','AvgSpeed']].astype(float)\n",
    "    \n",
    "    #computer rolling averages \n",
    "    for i in [3,5,10,14,21,30,45,60,90,120]: #set intervals for rolling average calculation\n",
    "        for j in ['Tmax', 'Tmin', 'Tavg', 'DewPoint', 'WetBulb', 'Heat', 'Cool',    #columns to compute rolling averages for\n",
    "                  'PrecipTotal', 'StnPressure', 'SeaLevel','ResultSpeed', 'ResultDir', 'AvgSpeed']:\n",
    "            df_total[j + str(i)] = df_total[j].rolling(window=i, min_periods=int(i/5)).mean() #create column with interval for each computation\n",
    "                                                                                #min_periods allows for computing these and not getting NaN values\n",
    "                                                                                #for reasonable amount of missing data\n",
    "    ## dummy the object variable\n",
    "    df_total = pd.get_dummies(df_total,drop_first=True,columns=['Species', 'month_day'])\n",
    "    \n",
    "    #drop'month_day_6.0', not in test data \n",
    "    df_total.drop(['month_day_6.0'], axis=1, inplace=True)\n",
    "    \n",
    "    #fill na's\n",
    "    df_total.fillna(method='backfill', inplace=True)\n",
    "    \n",
    "    \n",
    "    # return cleaned dataframe\n",
    "\n",
    "    return df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_test_data(df):\n",
    "    \n",
    "    # map df dataset with weather dataset\n",
    "    mask3 = np.power((df['Latitude']-41.995),2)+ np.power((df['Longitude']-(-87.933)),2) <= \\\n",
    "            np.power((df['Latitude']-41.786),2)+ np.power((df['Longitude']-(-87.752)),2)\n",
    "    mask4 = np.power((df['Latitude']-41.995),2)+ np.power((df['Longitude']-(-87.933)),2) > \\\n",
    "            np.power((df['Latitude']-41.786),2)+ np.power((df['Longitude']-(-87.752)),2)\n",
    "    df.loc[mask3,'weather_station'] = 1\n",
    "    df.loc[mask4,'weather_station'] = 2\n",
    "    df_weather = df.join(weather.set_index(['Date','Station']), on=(['Date','weather_station']), how='left')\n",
    "\n",
    "    # map df_weather dataset with spray dataset\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    date_formated_df = []\n",
    "    date_formated_spray = []\n",
    "    for i in df_weather['Date']:\n",
    "        date_formated_df.append(datetime.strptime(i, date_format))\n",
    "    for i in spray['Date']:\n",
    "        date_formated_spray.append(datetime.strptime(i, date_format))\n",
    "    date_formated_spray = set(date_formated_spray)\n",
    "    date_formated_spray = list(date_formated_spray)\n",
    "    days_after_spray = []\n",
    "    for i in date_formated_df:\n",
    "        delta = []\n",
    "        for n in date_formated_spray:\n",
    "            if (i - n).days > 0:\n",
    "                delta.append((i - n).days)\n",
    "        if len(delta) != 0:\n",
    "            days = np.min(delta)\n",
    "        else:\n",
    "            days = 0\n",
    "        days_after_spray.append(days)\n",
    "    df_weather['days_after_spray'] = days_after_spray\n",
    "    \n",
    "    ## deal with the missing value of the useful columns\n",
    "    mask5 = (df_weather.PrecipTotal == '  T')| (df_weather.PrecipTotal =='M')\n",
    "    mask6 = df_weather.WetBulb == 'M'\n",
    "    mask7 = df_weather.StnPressure == 'M'\n",
    "    df_weather.loc[mask5,'PrecipTotal'] = 0.00\n",
    "    df_weather.loc[mask6,'WetBulb'] = round((df_weather[df_weather.WetBulb!='M']['WetBulb']).astype('int').mean())\n",
    "    df_weather.loc[mask7,'StnPressure'] = round((df_weather[df_weather.StnPressure!='M']['StnPressure']).astype('float').mean(),2)\n",
    "    \n",
    "    #create columns for light and heavy rain day off and within 14 days \n",
    "    df_weather['PrecipTotal'] = df_weather['PrecipTotal'].astype(float)\n",
    "    precip_75quantile = df_weather[df_weather['PrecipTotal'] > 0]['PrecipTotal'].quantile(.75)\n",
    "    precip_25quantile = df_weather[df_weather['PrecipTotal'] > 0]['PrecipTotal'].quantile(.25)\n",
    "\n",
    "    df_weather['heavy_rain'] = (df_weather['PrecipTotal']>precip_75quantile).astype(int)\n",
    "    df_weather['light_rain'] = ((df_weather['PrecipTotal']>0) & (df_weather['PrecipTotal']<precip_25quantile)).astype(int)\n",
    "\n",
    "    for i in range(1,15):\n",
    "        df_weather['light_rain_past' + str(i)] = df_weather['light_rain'].shift(periods=i)\n",
    "        df_weather['heavy_rain_past' + str(i)] = df_weather['heavy_rain'].shift(periods=i)\n",
    "    \n",
    "    #create column if it rained in the past 14 days \n",
    "    df_weather['heavy_rain_last_14_days'] = np.where(df_weather[['heavy_rain_past1', 'heavy_rain_past2',\n",
    "       'heavy_rain_past3', 'heavy_rain_past4', 'heavy_rain_past5',\n",
    "       'heavy_rain_past6', 'heavy_rain_past7', 'heavy_rain_past8',\n",
    "       'heavy_rain_past9', 'heavy_rain_past10', 'heavy_rain_past11',\n",
    "       'heavy_rain_past12', 'heavy_rain_past13', 'heavy_rain_past14']].any(axis=1), 1, 0)\n",
    "\n",
    "    df_weather['light_reain_last_14_days'] = np.where(df_weather[['light_rain_past1', 'light_rain_past2',\n",
    "       'light_rain_past3', 'light_rain_past4', 'light_rain_past5',\n",
    "       'light_rain_past6', 'light_rain_past7', 'light_rain_past8',\n",
    "       'light_rain_past9', 'light_rain_past10', 'light_rain_past11',\n",
    "       'light_rain_past12', 'light_rain_past13', 'light_rain_past14',]].any(axis=1), 1, 0)\n",
    "    \n",
    "    # clean up the total dataset \n",
    "    df_total = df_weather\n",
    "    \n",
    "    #create ordinal date column\n",
    "    df_total['Date'] = pd.to_datetime(df_total['Date'])\n",
    "    df_total['ordinal_date'] = df_total['Date'].map(lambda x: x.toordinal())\n",
    "    \n",
    "    #create month/day for time of year \n",
    "    df_total['month_day'] = df_total['Date'].dt.month + df_total['Date'].map(lambda x: .5 if x.day > 15 else 0)\n",
    "    \n",
    "    ## drop useless columns\n",
    "    ## different for train and test dataset because test data doesn't include 'NumMosquitos' column\n",
    "    ## and test data has \"Id\" column that need to be dropped\n",
    "    \n",
    "    ### for train dataset\n",
    "#     df_total = df_total.drop(['Date','Address','Block','Street','Trap','AddressNumberAndStreet',\\\n",
    "#               'AddressAccuracy','weather_station','Depart','CodeSum','Depth',\\\n",
    "#               'Water1','SnowFall','Sunrise','Sunset','NumMosquitos'],axis=1)\n",
    "                 \n",
    "                 # NumMosquitos and CodeSum may still helpful (test data doesn't include 'NumMosquitos' column)\n",
    "    \n",
    "    ### for test dataset\n",
    "    df_total = df_total.drop(['Id','Date','Address','Block','Street','Trap','AddressNumberAndStreet',\\\n",
    "               'AddressAccuracy','weather_station','Depart','CodeSum','Depth',\\\n",
    "               'Water1','SnowFall','Sunrise','Sunset'],axis=1)\n",
    "    \n",
    "    \n",
    "    ## change the datatype to correct one\n",
    "    df_total[['Tavg','WetBulb','Heat','Cool']] = df_total[['Tavg','WetBulb','Heat','Cool']].astype(int)\n",
    "    df_total[['PrecipTotal','StnPressure','SeaLevel','AvgSpeed']] = \\\n",
    "    df_total[['PrecipTotal','StnPressure','SeaLevel','AvgSpeed']].astype(float)\n",
    "    \n",
    "    #computer rolling averages \n",
    "    for i in [3,5,10,14,21,30,45,60,90,120]: #set intervals for rolling average calculation\n",
    "        for j in ['Tmax', 'Tmin', 'Tavg', 'DewPoint', 'WetBulb', 'Heat', 'Cool',    #columns to compute rolling averages for\n",
    "                  'PrecipTotal', 'StnPressure', 'SeaLevel','ResultSpeed', 'ResultDir', 'AvgSpeed']:\n",
    "            df_total[j + str(i)] = df_total[j].rolling(window=i, min_periods=int(i/5)).mean() #create column with interval for each computation\n",
    "                                                                                #min_periods allows for computing these and not getting NaN values\n",
    "                                                                                #for reasonable amount of missing data\n",
    "    #fill na's\n",
    "    df_total.fillna(method='backfill', inplace=True)\n",
    "\n",
    "    ## dummy the object variable\n",
    "    df_total = pd.get_dummies(df_total,drop_first=True,columns=['Species', 'month_day'])\n",
    "    \n",
    "    \n",
    "    ## drop one of the dummy variable 'Species_UNSPECIFIED CULEX' that train dataset doesn't have\n",
    "    df_total = df_total.drop(['Species_UNSPECIFIED CULEX'],axis=1)\n",
    "    \n",
    "    # create polynomial features\n",
    "    \n",
    "    ## for train dataset\n",
    "#     y = df_total['WnvPresent']\n",
    "#     X = df_total.drop('WnvPresent',axis=1)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.15)\n",
    "#     poly = PolynomialFeatures(include_bias=False,degree=2)\n",
    "#     X_train_poly = poly.fit_transform(X_train)\n",
    "#     X_test_poly = poly.transform(X_test)\n",
    "#     X_train=pd.DataFrame(X_train_poly,columns=poly.get_feature_names(X_train.columns))\n",
    "#     X_test=pd.DataFrame(X_test_poly,columns=poly.get_feature_names(X_test.columns))\n",
    "    \n",
    "    ## for test dataset\n",
    "#     poly = PolynomialFeatures(include_bias=False,degree=2)\n",
    "#     df_total_poly = poly.fit_transform(df_total)\n",
    "#     df_total=pd.DataFrame(df_total_poly,columns=poly.get_feature_names(df_total.columns))\n",
    "    \n",
    "    # return cleaned dataframe\n",
    "    \n",
    "    ## for train dataset, return X_train and X_test data\n",
    "#     return X_train, X_test\n",
    "    \n",
    "    ## for test dataset, return df_total\n",
    "    return df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116293, 193)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean = clean_test_data(test)\n",
    "test_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8803, 195)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean = clean_train_data(df)\n",
    "train_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def humidity(Tf, Tdf):\n",
    "\n",
    "    # convert temp to celsius\n",
    "    Tc=5.0/9.0*(Tf-32.0)\n",
    "    # convert dewpoint temp to celsius\n",
    "    Tdc=5.0/9.0*(Tdf-32.0)\n",
    "    # saturation vapor pressure\n",
    "    Es=6.11*10.0**(7.5*Tc/(237.7+Tc))\n",
    "    # actual vapor pressure\n",
    "    E=6.11*10.0**(7.5*Tdc/(237.7+Tdc))\n",
    "    #relative humidity\n",
    "    RH =(E/Es)*100\n",
    "        \n",
    "    return RH\n",
    "\n",
    "train_clean[\"RelHum\"] = humidity(train_clean['Tavg'],train_clean['DewPoint'])\n",
    "test_clean[\"RelHum\"] = humidity(test_clean['Tavg'],test_clean['DewPoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [cols for cols in train_clean.columns if cols not in test_clean.columns] == ['WnvPresent', 'NumMosquitos_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.946723\n",
       "1    0.053277\n",
       "Name: WnvPresent, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean['WnvPresent'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# project nummosquitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create x and y columns for num mosquitos; take log of nummosquitos\n",
    "col_list = ['NumMosquitos_sum', 'WnvPresent']\n",
    "X_mos = train_clean[[cols for cols in train_clean.columns if cols not in col_list]]\n",
    "y_mos = np.log(train_clean['NumMosquitos_sum'])\n",
    "\n",
    "#TTS\n",
    "X_train_mos, X_test_mos, y_train_mos, y_test_mos = train_test_split(X_mos, y_mos, random_state=40, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_xgb = Pipeline([\n",
    "    ('ss', StandardScaler()),\n",
    "    ('xgb', xgb.XGBRegressor(reg_lambda=0))\n",
    "    \n",
    "])\n",
    "\n",
    "params_xgb = {\n",
    "    'xgb__n_estimators':[125,150, 175],\n",
    "    'xgb__learning_rate': [.1, .15, .2],\n",
    "    'xgb__max_depth': [3,5,7],\n",
    "    'xgb__reg_lambda': [0,1],\n",
    "    'xgb__reg_alpha': [0,1]\n",
    "}\n",
    "gs2 = GridSearchCV(pip_xgb,param_grid=params_xgb)\n",
    "gs2.fit(X_train_mos,y_train_mos)\n",
    "print('best cv score', gs2.best_score_)\n",
    "print('best paramas', gs2.best_params_)\n",
    "print('test score', gs2.score(X_test_mos, y_test_mos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot predictions vs actual on test set \n",
    "test_predictions_mos = np.exp(gs2.predict(X_test_mos))\n",
    "sns.jointplot(test_predictions_mos, y_test_mos, kind='reg', data= train_clean, size=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model on all training data\n",
    "mos_model = gs2.best_estimator_\n",
    "mos_model.fit(X_mos, y_mos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict mosquito counts for test data\n",
    "test_clean['project_mos'] = np.exp(mos_model.predict(test_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean['project_mos'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict mosquito counts for train data \n",
    "train_predicitions = np.exp(mos_model.predict(X_mos))\n",
    "train_clean['project_mos'] = train_predicitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle/unpickle model below and make predicitions\n",
    "# modelfilename= 'mos_model.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(mos_model, open(modelfilename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = pickle.load(open('mos_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model.score(X_test_mos, y_test_mos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predict mosquito counts for test data\n",
    "# test_clean['project_mos'] = loaded_model.predict(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predict mosquito counts for train data \n",
    "# train_clean['project_mos'] = loaded_model.predict(train_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the classficiation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "## Separate majority and minority classes\n",
    "df_majority = train_clean[train_clean.WnvPresent==0]\n",
    "df_minority = train_clean[train_clean.WnvPresent==1]\n",
    " \n",
    "## Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=8334,    # to match majority class\n",
    "                                 random_state=42) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.WnvPresent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_upsampled['WnvPresent']\n",
    "X = df_upsampled.drop(['WnvPresent', 'NumMosquitos_sum'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest is OK\n",
    "pip1 = Pipeline([\n",
    "    ('rf', RandomForestClassifier()) \n",
    "])\n",
    "params1 = {\n",
    "    'rf__n_estimators':[5,10,20],\n",
    "    'rf__max_features': [\"auto\",\"sqrt\",\"log2\"],\n",
    "    'rf__max_depth': [None,10,20]\n",
    "}\n",
    "gs3 = GridSearchCV(pip1,param_grid=params1)\n",
    "gs3.fit(X_train,y_train)\n",
    "print('best cv score', gs3.best_score_)\n",
    "print('best paramas', gs3.best_params_)\n",
    "print('test score', gs3.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "y_hat = gs3.predict(X_test)\n",
    "confusion_matrix(y_test,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs3.predict(X_test)\n",
    "FPR, TPR, threshold = roc_curve(y_test, y_pred)\n",
    "ROC_AUC = auc(FPR, TPR)\n",
    "\n",
    "plt.figure(figsize=[11,9])\n",
    "plt.plot(FPR, TPR, label='ROC curve (area = %0.2f)' % ROC_AUC, linewidth=4)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate (Recall)', fontsize=18)\n",
    "plt.ylabel('True Positive Rate (Precision)', fontsize=18)\n",
    "plt.title('LogReg for \"CKD\"', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs3.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = gs3.predict(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pd.DataFrame(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred['Id'] = [i for i in range(1,116294)]\n",
    "test_pred['WnvPresent'] = test_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.drop([0],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_pred['WnvPresent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.to_csv('test_pred_15.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split \n",
    "y = train_clean['WnvPresent']\n",
    "X = train_clean.drop(['WnvPresent', 'NumMosquitos_sum'], axis=1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    stratify = y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "\n",
    "# Using RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state=19)\n",
    "bbc = BalancedBaggingClassifier(base_estimator=rfc,\n",
    "                                random_state=10)\n",
    "bbc.fit(X_train,y_train)\n",
    "\n",
    "bbc_predict = bbc.predict(X_test)\n",
    "bbc_score = recall_score(y_test, bbc_predict)\n",
    "\n",
    "print (\"BBC Test Score:\", bbc_score)\n",
    "\n",
    "bbc_confusion = pd.DataFrame(np.array(confusion_matrix(y_test, bbc_predict)),\n",
    "                         index = ['is_not_abnormal', 'is_abnormal'],\n",
    "                        columns=['predicted_not_abnormal','predicted_abnormal'])\n",
    "bbc_confusion\n",
    "\n",
    "bbc_params = {\n",
    "    'n_estimators':np.arange(5,20),\n",
    "    'warm_start': [True, False]\n",
    "#     'max_samples': [1,2]\n",
    "#     'max_features': np.arange(1,10)\n",
    "}\n",
    "\n",
    "bbc_gs = GridSearchCV(bbc,\n",
    "                     param_grid=bbc_params,\n",
    "                     scoring='recall',\n",
    "                     cv = 10)\n",
    "\n",
    "bbc_gs.fit(X_train,y_train)\n",
    "bbc_gs_score = bbc_gs.score(X_test, y_test)\n",
    "\n",
    "print(\"Best Params: \", bbc_gs.best_params_)\n",
    "print (\"BBC Test Score:\", bbc_gs_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_gs.fit(X,y)\n",
    "test_pred2 = bbc_gs.predict(test_clean)\n",
    "test_pred2 = pd.DataFrame(test_pred2)\n",
    "test_pred2['Id'] = [i for i in range(1,116294)]\n",
    "test_pred2['WnvPresent'] = test_pred2[0]\n",
    "test_pred2.drop([0],axis=1,inplace=True)\n",
    "test_pred2['WnvPresent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred2.to_csv('test_pred_16.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Predict test data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# additional models (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # svm\n",
    "# gamma_range = [0.001]\n",
    "# C_range = [1]\n",
    "# kernel_range = ['rbf', 'sigmoid', 'linear', 'poly']\n",
    "\n",
    "# params3 = dict(gamma=gamma_range, kernel=kernel_range, C=C_range)\n",
    "\n",
    "# clf = svm.SVC()\n",
    "# gs3 = GridSearchCV(clf, params3, scoring=\"accuracy\", verbose=0)\n",
    "# gs3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('best cv score', gs3.best_score_)\n",
    "# print('best paramas', gs3.best_params_)\n",
    "# print('test score', gs3.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # svm using unbalanced sample and use class_weight = 'WnvPresent' to penalize mistakes on the minority class \n",
    "# # by an amount proportional to how under-represented it is.\n",
    "# y = train_clean['WnvPresent']\n",
    "# X = train_clean.drop('WnvPresent',axis=1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.15)\n",
    "\n",
    "# gamma_range = [0.001]\n",
    "# C_range = [1]\n",
    "# kernel_range = ['linear']\n",
    "# class_weight=['balanced']\n",
    "\n",
    "# params4 = dict(gamma=gamma_range, kernel=kernel_range, C=C_range, class_weight=class_weight)\n",
    "\n",
    "# clf = svm.SVC()\n",
    "# gs4 = GridSearchCV(clf, params4, scoring=\"accuracy\", verbose=0)\n",
    "# gs4.fit(X_train,y_train)\n",
    "\n",
    "# print('best cv score', gs4.best_score_)\n",
    "# print('best paramas', gs4.best_params_)\n",
    "# print('test score', gs4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # logistic regression is bad?\n",
    "# pip2 = Pipeline([\n",
    "#     ('ss', StandardScaler()), \n",
    "#     ('lr', LogisticRegression())\n",
    "# ])\n",
    "# params2 = {\n",
    "#     'lr__penalty':['l1','l2'],\n",
    "#     'lr__C':[0.1,0.5,0.9]\n",
    "# }\n",
    "# gs2 = GridSearchCV(pip2,param_grid=params2)\n",
    "# gs2.fit(X_train,y_train)\n",
    "# print('best cv score', gs2.best_score_)\n",
    "# print('best paramas', gs2.best_params_)\n",
    "# print('test score', gs2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a confusion matrix\n",
    "# y_hat = gs2.predict(X_test)\n",
    "# confusion_matrix(y_test,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN using Tensorflow\n",
    "y = df_upsampled['WnvPresent']\n",
    "X = df_upsampled.drop(['WnvPresent', 'NumMosquitos_sum'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.15)\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None,X_train.shape[1]), name='X')\n",
    "y = tf.placeholder(dtype=tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "h1 = tf.layers.dense(X, 99, activation=tf.nn.relu, name='hidden1', kernel_regularizer=regularizers.l2(0.01)) # 20 features put into the layer\n",
    "h2 = tf.layers.dropout(h1, rate=0.5)\n",
    "h3 = tf.layers.dense(h2, 20, activation=tf.nn.relu, name='hidden2', kernel_regularizer=regularizers.l2(0.01))\n",
    "h4 = tf.layers.dropout(h3, rate=0.5)\n",
    "h5 = tf.layers.dense(h4, 20, activation=tf.nn.relu, name='hidden3', kernel_regularizer=regularizers.l2(0.01))\n",
    "h6 = tf.layers.dropout(h5, rate=0.5)\n",
    "h7 = tf.layers.dense(h6, 20, activation=tf.nn.relu, name='hidden4', kernel_regularizer=regularizers.l2(0.01))\n",
    "\n",
    "y_hat = tf.layers.dense(h4, 1, activation=tf.nn.sigmoid, name='y_hat') # this is a binary classification problem\n",
    "\n",
    "loss = tf.losses.log_loss(y, y_hat)\n",
    "optimizer = tf.train.AdamOptimizer(.01) # adam do gradient descent: learning rate is little bit larger\n",
    "training_run = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "train_errs=[]\n",
    "test_errs=[]\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(1100):\n",
    "        sess.run(training_run, feed_dict={X: X_train, y: y_train})\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            train_loss = sess.run(loss, feed_dict={X: X_train, y: y_train})\n",
    "            train_errs.append(train_loss)\n",
    "            test_loss = sess.run(loss, feed_dict={X: X_test, y: y_test})\n",
    "            test_errs.append(test_loss)\n",
    "            print('epoch', epoch, 'Train loss', train_loss, 'Test loss', test_loss)\n",
    "    saver.save(sess, './Classfication.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the learning curves\n",
    "plt.plot(train_errs, label = 'Train loss')\n",
    "plt.plot(test_errs, label = 'Test loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './Classfication.ckpt')\n",
    "    pred = sess.run(y_hat, feed_dict={X:X_test}) # run the prediction layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (pred > .947).astype(int)\n",
    "metrics.accuracy_score(y_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPR, TPR, threshold = roc_curve(y_test, pred)\n",
    "ROC_AUC = auc(FPR, TPR)\n",
    "\n",
    "plt.figure(figsize=[11,9])\n",
    "plt.plot(FPR, TPR, label='ROC curve (area = %0.2f)' % ROC_AUC, linewidth=4)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate (Recall)', fontsize=18)\n",
    "plt.ylabel('True Positive Rate (Precision)', fontsize=18)\n",
    "plt.title('LogReg for \"CKD\"', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "test_clean = ss.fit_transform(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './Classfication.ckpt')\n",
    "    pred3 = sess.run(y_hat, feed_dict={X:test_clean}) # run the prediction layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = pd.DataFrame(pred3)\n",
    "pred3['Id'] = [i for i in range(1,116294)]\n",
    "pred3['WnvPresent'] = pred3[0]\n",
    "pred3.drop([0],axis=1,inplace=True)\n",
    "# pred3['WnvPresent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = pred3['WnvPresent'] >0.8\n",
    "pred3.loc[mask, 'WnvPresent'] = int(1)\n",
    "pred3.loc[~mask, 'WnvPresent'] = int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3['WnvPresent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3.to_csv('test_pred_14.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
